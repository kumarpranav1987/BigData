Sqoop Import is used to import data from RDBMS to HDFS  

sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password password --table order_items --target-dir /retail_db  

Observations (Look in the logs)
Sqoop Import Execution Life Cycle  
Sqoop import is excuted as a MapReduce job.  
To generate the code for map reduce sqoop needs the schema of the table so it fires a select quuery  
in our case 
INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `order_items` AS t LIMIT 1  
Using this schema the required MapReduce code is generated compiled and bundeled into a jar for execution
By default Sqoop uses 4 mappers for import, can be changed using --num-mappers  
sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password password --table order_items --num-mappers 1 --target-dir /retail_db

As the defaut number of mapper is 4 so four output files will be generated in the specified target directory.
Whole data is divided into 4 mutualy exclusive parts and each mapper work on one part.
For eaxmple if there are 100 rows in table then each mapper will work on 25 rows.
This division is done using below query  

INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`order_item_id`), MAX(`order_item_id`) FROM `order_items`
INFO db.IntegerSplitter: Split size: 43049; Num splits: 4 from: 1 to: 172198
INFO mapreduce.JobSubmitter: number of splits:4



1)Target HDFS path has been specified using --target-dir so the data is imported in this directory  
We can also pass the target HDFS path using --warehouse-dir, in this case a subdirectory with the name of table is created in the passed directory and data is copied inside that.  


sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password password --table order_items --warehouse-dir /retail_db  

2)Target HDFS directory should be empty otherwise import will fail.

3)Default field deliminator is comma ,


