Sqoop Import is used to import data from RDBMS to HDFS  

sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password password --table order_items --target-dir /retail_db  

Observations (Look in the logs)
Sqoop Import Execution Life Cycle  
Sqoop import is excuted as a MapReduce job.  
To generate the code for map reduce sqoop needs the schema of the table so it fires a select query  
in our case 
INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `order_items` AS t LIMIT 1  

Using this schema the required MapReduce code is generated compiled and bundeled into a jar for execution
By default Sqoop uses 4 mappers for import, can be changed using --num-mappers  
sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password password --table order_items --num-mappers 1 --target-dir /retail_db

As the defaut number of mapper is 4 so four output files will be generated in the specified target directory.
Whole data is divided into 4 mutualy exclusive parts using the primary key and each mapper work on one part.
For eaxmple if there are 100 rows in table then each mapper will work on 25 rows.
This division is done using below query  (In this case primary key is order_item_id)

INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`order_item_id`), MAX(`order_item_id`) FROM `order_items`
INFO db.IntegerSplitter: Split size: 43049; Num splits: 4 from: 1 to: 172198
INFO mapreduce.JobSubmitter: number of splits:4

If the table does not have any primary key then import will fail(if number of mapper is not 1)because it does not know how to
partion the tasks between mappers.In this scenario we can specify a column for spliting the data using the control argument 
--split-by <column name>
sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password password --table Test --target-dir /Test --split-by age  


Points to remember while choosing the split by column  
--It should be indexed other wise query time will be very large  
--It should be Sequence Genrated or Evenly Generated
--It should not have null values otherwise those rows will be ignored

By Default spliting by non numeric field is not allowed but it can be overriden by passing following VM argument
-Dorg.apache.sqoop.splitter.allow_text_splitter=true

sqoop import -Dorg.apache.sqoop.splitter.allow_text_splitter=true --connect jdbc:mysql://localhost/retail_db --username root --password password --table Test --split-by name --target-dir /Test

###
--autoreset-to-one-mapper 	Import should use one mapper if a table has no primary key and no split-by column is provided. Cannot be used with --split-by <col> option.

1)Target HDFS path has been specified using --target-dir so the data is imported in this directory  
We can also pass the target HDFS path using --warehouse-dir, in this case a subdirectory with the name of table is created in the passed directory and data is copied inside that.  


sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password password --table order_items --warehouse-dir /retail_db  

2)Target HDFS directory should be empty otherwise import will fail.To overwrite the target directory if it exists then we should pass the control argument --target-dir
sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password password --table order_items  --target-dir /retail_db --delete-target-dir  

if we want to append in the taget directory we can use --append

sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password password --table order_items  --target-dir \  
/retail_db --delete-target-dir 
3)Default field deliminator is comma ,

4)File Format
Default File format is text file , we can also specify it using --as-textfile
Other File formats
--as-avrodatafile
--as-sequencefile
--as-parquetfile
5)Compressing the imported data
--compress 	Enable compression (Defaut Compression id gzip)
sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password password --table order_items --target-dir /retail_db --compress


To specify compression codec (Supported codecs can be checked from $HADOOP_HOME/etc/hadoop/core-site.xml property name is io.compression.codec)
--compression-codec <c> 	Use Hadoop codec (default gzip)

### Boundary query : To divide the task among mappers sqoop runs a select query to find min and max value of primary key(or column specified by --split-by)and then divides the range among mappers.
If we want to customorize this we can do this using --boundary-query Like lets say we want to import only data where order_item_id is 6 digits then
sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password password --table order_items --target-dir /retail_db --boundary-query 'select min(order_item_id), max(order_item_id) from order_items where order_item_id > 99999'

We can even hard code the valuses in boundary query
sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password password --table order_items --target-dir /retail_db --boundary-query 'select 99999,172198'

### Importing specific columns and filtering data 
--table <table name> --columns <col,col,col....>
sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password password --table order_items --columns order_item_id,order_item_subtotal --target-dir /retail_db 

Filterng and JOINing tables(--splitby is mandatory as we dont use --table while using --query)
--table and/or --columns is mutually exclusive with --query
Meaning we cant use them together

sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password password --query 'select o.*,oi.order_item_id from orders o join order_items oi on (o.order_id = oi.order_item_order_id) WHERE $CONDITIONS'  --split-by order_id --target-dir /retail_db
