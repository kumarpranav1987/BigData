Sqoop Import is used to import data from RDBMS to HDFS  

sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password password --table order_items --target-dir /retail_db  

Observations (Look in the logs)
Sqoop Import Execution Life Cycle  
Sqoop import is excuted as a MapReduce job.  
To generate the code for map reduce sqoop needs the schema of the table so it fires a select query  
in our case 
INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `order_items` AS t LIMIT 1  

Using this schema the required MapReduce code is generated compiled and bundeled into a jar for execution
By default Sqoop uses 4 mappers for import, can be changed using --num-mappers  
sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password password --table order_items --num-mappers 1 --target-dir /retail_db

As the defaut number of mapper is 4 so four output files will be generated in the specified target directory.
Whole data is divided into 4 mutualy exclusive parts using the primary key and each mapper work on one part.
For eaxmple if there are 100 rows in table then each mapper will work on 25 rows.
This division is done using below query  (In this case primary key is order_item_id)

INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`order_item_id`), MAX(`order_item_id`) FROM `order_items`
INFO db.IntegerSplitter: Split size: 43049; Num splits: 4 from: 1 to: 172198
INFO mapreduce.JobSubmitter: number of splits:4

If the table does not have any primary key then import will fail(if number of mapper is not 1)because it does not know how to
partion the tasks between mappers.In this scenario we can specify a column for spliting the data using the control argument 
--split-by <column name>
sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password password --table Test --target-dir /Test --split-by age  


Points to remember while choosing the split by column  
--It should be indexed other wise query time will be very large  
--It should be Sequence Genrated or Evenly Generated


1)Target HDFS path has been specified using --target-dir so the data is imported in this directory  
We can also pass the target HDFS path using --warehouse-dir, in this case a subdirectory with the name of table is created in the passed directory and data is copied inside that.  


sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password password --table order_items --warehouse-dir /retail_db  

2)Target HDFS directory should be empty otherwise import will fail.To overwrite the target directory if it exists then we should pass the control argument --target-dir
sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password password --table order_items  --target-dir /retail_db --delete-target-dir  

if we want to append in the taget directory we can use --append

sqoop import --connect jdbc:mysql://localhost/retail_db --username root --password password --table order_items  --target-dir \  
/retail_db --delete-target-dir 
3)Default field deliminator is comma ,


